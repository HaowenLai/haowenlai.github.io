<!DOCTYPE html>
<html class="mozwebext" lang="en">
	<head>
		<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
		<meta http-equiv="Content-Script-Type" content="text/javascript">
		<meta http-equiv="Content-Style-Type" content="text/css">
		
		<title>Haowen Lai</title>
		<link rel="shortcut icon" href="images/favicon.ico">
		<link rel="stylesheet" type="text/css" href="css/css.css">
		<link rel="stylesheet" type="text/css" href="css/style.css">
		<script src="css/hidebib.js" type="text/javascript"></script>
		<!--[if !IE]>-->
		<link rel="stylesheet" type="text/css" media="only screen and (max-device-width: 480px)" href="css/iphone.css" >
		<link rel="stylesheet" type="text/css" href="css/css_002.css" >
		<meta name="viewport" content="width=device-width; initial-scale=1.0; maximum-scale=1.0; user-scalable=0;">
	</head>
	
<body>
<div id="wrap">
	<div id="photo"><img src="images/self.png" alt="self"></div>
      <div id="profile" valign="top">
             <p class="bigskip"></p>
              <h1>Haowen Lai</h1>
              <p class="bigskip"></p>
              <p>Master Student</p>
              <p><a href="http://www.au.tsinghua.edu.cn/publish/auen/index.html">Department of Automation</a></p>
              <p><a href="https://www.tsinghua.edu.cn/en/">Tsinghua University</a></p>
              <p>Beijing, China, 100084</p>
              <p class="bigskip"></p>
              <p>lhw19@mails.tsinghua.edu.cn</p>
              <div class="clear"></div>
	</div>
	<!-- end of top -->

	<div id="menu">
		<div><a href="index.html">Home</a></div>
		<div><a href="bio.html">Bio</a></div>
		<div class="now"><a href="research.html">Research</a></div>
		<div><a href="publications.html">Publications</a></div>
	</div>
	<!-- end of memu --> 
	
	<div id="projects" class="section">
	<h2>Projects</h2>
      <table width="100%" cellspacing="0" cellpadding="10" border="0" align="center"><tbody>
	<tr>
      	<td width="30%" valign="top" align="center">
          		<img src="images/proj_mapMerge.png" alt="proj_mapMerge" style="border-style: none"  width="90%">
          	</td>
		<td width="70%" valign="top">
			<b>AutoMerge: Automatic Multi-agent Cooperated Map Merging System</b>
			<br><a href="http://theairlab.org">AirLab</a>, Carnegie Mellon University
			<br>(as <u><i>Research Intern</i></u>, advisor: prof. <a href="https://www.ri.cmu.edu/ri-faculty/sebastian-scherer/">Sebastian Scherer</a>)
			<br><span class="date">Dec. 2021 - present</span><br>
			<div class="project_link" id="projMapMerge">[<a href="javascript:toggleblock('projMapMerge_abs')">abstract</a>]</div><br>
			<div class="project_abs"><p id="projMapMerge_abs" style="display: none;">
				The project focuses on merging large-scale 3D point cloud maps without any prior information about their relative 
				transformation. These large-scale point clouds, which are usually generated using SLAM with 3D LiDARs in the outdoors, differ 
				a lot from the ones created by 3D scanners or RGB-D cameras. The noise becomes larger and the points are sparser, making 
				most descriptors designed for point cloud registration fail. Furthermore, the issue of time consumption must be taken into 
				consideration as the scale of maps becomes increasingly larger. To handle it, we propose a novel method that selects key points
				at a higher sematic level rather than choose them randomly as most current works do. Then features are extracted and 
				descriptors are constructed by taking their neighbors into account. For details, please refer to our paper.
			</p></div>
		</td>
	</tr>
	
	<tr>
      	<td width="30%" valign="top" align="center">
          		<img src="images/proj_LocPercep.png" alt="proj_LocPercep" style="border-style: none"  width="90%">
          	</td>
		<td width="70%" valign="top">
			<b>LiDAR Inertial based Self-Localization and Target Perception for Robots</b>
			<br>UAV Lab, Tsinghua University (as <u><i>researcher</i></u>, advisor: prof. Yisheng Zhong)
			<br><span class="date">Jan. 2021 - Jul. 2021</span><br>
			<div class="project_link" id="projLocPercep">[<a href="javascript:toggleblock('projLocPercep_abs')">abstract</a>]
			                                                                      [<a href="publications.html#paperCCC2021">paper</a>]
														[<a href="publications.html#patent2021">patent</a>]</div><br>
			<div class="project_abs"><p id="projLocPercep_abs" style="display: none;">
				The abilities of self-localization and target perception for robots are of vital importance since they are the basic of other 
				upper level functions, such as path planning, obstacle avoidance and navigation. Although some methods can provide 
				the robot with localization results directly, they are somehow with weaknesses. For example, the use of external visual 
				localization system may be unrealistic in practice, and GPS/RTK is rarely working in indoor enviroment. Thus, robots should 
				be able to locate themselves with the sensors they carry. The capability of target perception is also signifiant in some tasks 
				such as searching, game playing and autonomous driving. It is unreasonable to expect information sharing from the opponents 
				in real scenes like most theory analyses do. In this project, we leverage 3D LiDAR and IMU to provide robots with the abilities of 
				self-localization and target perception in pursuit-evasion games. The results show that our method is accurate and effective 
				in both indoor and outdoor enviroment. For more details, please refer to our papers.
			</p></div>
		</td>
	</tr>
      
      <tr>
      	<td width="30%" valign="top" align="center">
          		<img src="images/proj_SLAM_mapping.png" alt="project_SLAM_mapping" style="border-style: none"  width="90%">
          	</td>
		<td width="70%" valign="top">
			<b>Intelligent and Unmanned Indoor Navigation Robot</b>
			<br>UAV Lab, Tsinghua University (as <u><i>researcher</i></u>, advisor: prof. Yisheng Zhong)
			<br><span class="date">Jan. 2020 - Sept. 2020</span><br>
			<div class="project_link" id="projSLAMMapping">[<a href="javascript:toggleblock('projSLAMMapping_abs')">abstract</a>]
			                                                                             [<a href="publications.html#paperCDC2021">paper</a>]</div><br>
			<div class="project_abs"><p id="projSLAMMapping_abs" style="display: none;">
				The research in this project focuses on robot sensing, understanding and interaction with the unknown environment by itself. Some key technologies are 
				utilized, such as SLAM, multi-sensor fusion, robot navigation, self-exploration, object detection etc. What we want to do is making robots as intelligent as
				human in understanding its enviroment, especially in an unknown area with no human instruction. Specific tasks include exploring the enviroment autonomously and
				building a map, dynamic obstacles avoidance, self localization, object picking by robot arm, etc.
			</p></div>
		</td>
	</tr>
      
      <tr>
      	<td width="30%" valign="top" align="center">
          		<img src="images/proj_meterReader.png" alt="project_meter_reader" style="border-style: none"  width="90%">
          	</td>
		<td width="70%" valign="top">
			<b>Vision-based Automatic Reading System for Pointer Meters</b>
			<br><a href="https://github.com/Tongji-BitaAI-Lab">BitaAI Lab, Tongji University</a> (as <u><i>main researcher</i></u>, advisor: prof. Qi Kang)
			<br><span class="date">Nov. 2018 - Aug. 2019</span><br>
			<div class="project_link" id="projMeterReader">[<a href="javascript:toggleblock('projMeterReader_abs')">abstract</a>]
													           [<a href="publications.html#paper2019">paper</a>] 
														    [<a href="publications.html#patent2019">patent</a>]</div><br>
			<div class="project_abs"><p id="projMeterReader_abs" style="display: none;">
				This project aims to build an automatic reading system for pointer meters. The system is comprehensive, including image capture device, core reading algorithm and reading
				management GUI. Unlike other reading systems, the core reading algorithm applied in ours was proposed by our team (see [<a href="publications.html">pub</a>]), which is able
				to process both uniform and non-uniform scale meters (meters whose scales are unevenly distributed) regardless of their shapes. This ability undoubtedly enlarges the range of
				meters in application. In some circumstances such as power plant, it is even critical because most of their meters are of non-uniform scales, e.g. power factor meter and large
				measurement Ampere meter. Images are captured by an self-designed embedded device and transmitted through wireless network. Besides, reading management GUI is also provided
				for monitoring, recording and controlling the whole system.
			</p></div>
		</td>
	</tr>
      
      <tr>
      	<td width="30%" valign="top" align="center">
          		<img src="images/proj_rubikCube.png" alt="project_rubik's_cube" style="border-style: none"  width="70%">
          	</td>
		<td width="70%" valign="top">
			<b>Fast and Robust Rubik's Cube Solver with Illumination Adaptability</b>
			<br><u><i>Haowen Lai</i></u>, Jinshu Chen, Bei Zhang (advisor: prof. Youling Yu)
			<br><span class="date">Sept. 2018 - Jan. 2019</span><br>
			<div class="project_link" id="projRubikCube">[<a href="javascript:toggleblock('projRubikCube_abs')">abstract</a>]
														[<a href="https://github.com/HaowenLai/rubikCube">code</a>] 
			<div class="project_abs"><p id="projRubikCube_abs" style="display: none;">
				In this project, we made a robot (though it could not walk or move) that can restore 3-order Rubik's Cube and is robust to the enviromental illumination changes. Its robustness results from 
				the color recognition algorithm that differs from traditional ones which simply apply threshold to HSV color space to segment color. The problem is that a fixed threshold cannot handle 
				various illumination conditions. To overcome the weakness, we apply machine learning techniques to enable the robot to learn the threshold according to its current enviroment. That improves 
				its adaptability and makes user get rid of parameters tuning all the time. Besides, the controlling sequence to the motors for restoration is also optimized, e.g. the opposite motors can move 
				at the same time without conflict, thus shortening the time used.
			</p></div>
		</td>
	</tr>
	
	<tr>
      	<td width="30%" valign="top" align="center">
          		<img src="images/proj_robotArm.png" alt="project_robot_arm" style="border-style: none"  width="90%">
          	</td>
		<td width="70%" valign="top">
			<b>Self-Learning Model-Free Robot Arm System for Grabbing and Classification</b>
			<br>Innovation Base Laboratory (as <u><i>main researcher</i></u> and <u><i>team leader</i></u>)
			<br>Competition: Intel Cup ESDC 2018. National first prize (top 8% of 164 teams).
			<br><span class="date">Jan. 2018 - Aug. 2018</span><br>
			<div class="project_link" id="projRobotArm">[<a href="javascript:toggleblock('projRobotArm_abs')">abstract</a>] 
															[<a href="https://github.com/HaowenLai/RobotArm1">code</a>] 
														      [<a href="publications.html#patent2018">patent</a>]</div><br>
			<div class="project_abs"><p id="projRobotArm_abs" style="display: none;">
				We presented a robot arm system for grabbing and classification in this project. As we know, for most control algorithms of robot arms, models need to be built before the arms can precisely 
				move to a certain position. It is not an easy job especially when your robot arms are of different types. Even for the same type, it is hard to acquire an accurate model due to the manufacturing 
				errors. So we use deep learning to make the arm learn the model by itself through practicing. The location of its joints acquired by the visual positioning module is used as training samples 
				together with the control values of each motors. To Illustrate the adaptability and effectiveness of our approach, the algorithm is applied to two different robot arms for grabbing and classification 
				tasks through collaboration. If the classification features are blocked by the clip of one arm when holding the object, the arm will pass it to another and let it observes the features.
			</p></div>
		</td>
	</tr>
	
	<tr>
      	<td width="30%" valign="top" align="center">
          		<img src="images/proj_selfieDrone.png" alt="project_selfie_drone" style="border-style: none"  width="80%">
          	</td>
		<td width="70%" valign="top">
			<b>Follow You up: Selfie UAV with Gesture Interaction</b>
			<br>National Innovation Project (as <u><i>team leader</i></u>, advisor: prof. Fanhuai Shi)
			<br><span class="date">Apr. 2017 - Apr. 2018</span><br>
			<div class="project_link" id="projSelfieDrone">[<a href="javascript:toggleblock('projSelfieDrone_abs')">abstract</a>]</div><br>
			<div class="project_abs"><p id="projSelfieDrone_abs" style="display: none;">
				This project focused on how to provide convenient UAV control and better user experience in selfie. In our research, the remote controller 
				is discarded. Instead, users control the UAV for a certain task such as stopping following, resuming and landing through hand gestures. 
				The gesture recognition module first segments hand area by color feature, then uses neural network to infer its meaning. The segmentation procedure 
				dramatically reduces the time compared to using CNN to infer on a 3-channals color image directly, and requires less training samples. 
				Another distinguishing characteristic is that our UAV utilizes a depth camera to locate the user and will automatically follow the person when moving, with 
				no worry of getting lost. Other functions include automatic finding best selfie position, group shots and video recording, etc.
			</p></div>
		</td>
	</tr>
	</tbody></table>
	</div>  
	<!-- end div of projects --> 
	
	
</div>  <!-- end div wrap   -->


</body></html>